---
title: 'Assignment #2'
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r libraries}
# SEE modeldata package for new datasets
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(stacks)            # for stacking models
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date manipulation
library(moderndive)        # for King County housing data
library(vip)               # for variable importance plots
library(DALEX)             # for model interpretation  
library(DALEXtra)          # for extension of DALEX
library(patchwork)         # for combining plots nicely
library(openintro)         # for loan data
theme_set(theme_minimal()) # Lisa's favorite theme
```

```{r data}
data("lending_club")
# Data dictionary (as close as I could find): https://www.kaggle.com/wordsforthewise/lending-club/discussion/170691
```


When you finish the assignment, remove the `#` from the options chunk at the top, so that messages and warnings aren't printed. If you are getting errors in your code, add `error = TRUE` so that the file knits. I would recommend not removing the `#` until you are completely finished.

## Put it on GitHub!        

From now on, GitHub should be part of your routine when doing assignments. I recommend making it part of your process anytime you are working in R, but I'll make you show it's part of your process for assignments.

**Task**: When you are finished with the assignment, post a link below to the GitHub repo for the assignment. If you want to post it to your personal website, that's ok (not required). Make sure the link goes to a spot in the repo where I can easily find this assignment. For example, if you have a website with a blog and post the assignment as a blog post, link to the post's folder in the repo. As an example, I've linked to my GitHub stacking material [here](https://github.com/llendway/ads_website/tree/master/_posts/2021-03-22-stacking).

## Modeling

Before jumping into these problems, you should read through (and follow along with!) the [model stacking](https://advanced-ds-in-r.netlify.app/posts/2021-03-22-stacking/) and [global model interpretation](https://advanced-ds-in-r.netlify.app/posts/2021-03-24-imlglobal/) tutorials on the Course Materials tab of the course website.

We'll be using the `lending_club` dataset from the `modeldata` library, which is part of `tidymodels`. The data dictionary they reference doesn't seem to exist anymore, but it seems the one on this [kaggle discussion](https://www.kaggle.com/wordsforthewise/lending-club/discussion/170691) is pretty close. It might also help to read a bit about [Lending Club](https://en.wikipedia.org/wiki/LendingClub) before starting in on the exercises.

The outcome we are interested in predicting is `Class`. And according to the dataset's help page, its values are "either 'good' (meaning that the loan was fully paid back or currently on-time) or 'bad' (charged off, defaulted, of 21-120 days late)".

**Tasks:** I will be expanding these, but this gives a good outline.

1. Explore the data, concentrating on examining distributions of variables and examining missing values. 

2. Do any data cleaning steps that need to happen before the model is build. For example, you might remove any variables that mean the same thing as the response variable (not sure if that happens here), get rid of rows where all variables have missing values, etc. 

3. Split the data into training and test, putting 75\% in the training data.

```{r}
set.seed(494) # for reproducibility
```

4. Set up the recipe and the pre-processing steps to build a lasso model. Some steps you should take:

* Make all integer variables numeric (I'd highly recommend using `step_mutate_at()` or this will be a lot of code). We'll want to do this for the model interpretation we'll do later.  
* Think about grouping factor variables with many levels.  
* Make categorical variables dummy variables (make sure NOT to do this to the outcome variable).  
* Normalize quantitative variables.  

5. Set up the lasso model and workflow.  

6. Set up the model tuning for the `penalty` parameter. Be sure to add the `control_stack_grid()` for the `control` argument so we can use these results later when we stack.  

7. Set up the recipe and the pre-processing steps to build a random forest model. You shouldn't have to do as many steps. The only step you should need to do is making all integers numeric. 

8. Set up the random forest model and workflow.  

9. Set up the model tuning for both the `mtry` and `min_n` parameters. Be sure to add the `control_stack_grid()` for the `control` argument so we can use these results later when we stack. 

10. Find the best tuning parameters. What is the are the accuracy and area under the ROC curve for those tuning parameters?

11. Use functions from the `DALEX` and `DALEXtra` libraries to create a histogram and boxplot of the residuals from the training data. How do they look? Any interesting behavior?  

12. Use `DALEX` functions to create a variable importance plot from this model. What are the most important variables? 

13. Write a function called `cp_profile` to make a CP profile. The function will take the output from a `predict_profile()` function and a variable name as its arguments and create a CP profile.

14. Use `DALEX` functions to create partial dependence plots for the 5-7 most important variables. If the important variables are categorical, you can instead make a CP profile for 3 observations in the dataset and discuss how you could go about constructing a partial dependence plot for a categorical variable (you don't have to code it, but you can if you want an extra challenge).

15. Fit one more model type of your choosing that will feed into the stacking model. 

16. Create a model stack with the candidate models from the previous parts of the exercise. 

## shiny app

## Coded Bias



