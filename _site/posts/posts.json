[
  {
    "path": "posts/2021-03-16-ml-review/",
    "title": "Machine Learning review with an intro to `tidymodels`",
    "description": {},
    "author": [],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\n\nContents\nResources\nReview\nUsing tidymodels for the process1. Quick exploration\n2. Data splitting\n3. Data pre-processing\n4. Fitting model(s)\n6. Evaluate & compare models\n7. Apply model to testing data\n9. Question the data and model\n8. Use the model\n5. Tuning model parameters\n\n\n\n\n\nResources\nHere are some great resources. I will reference some of them throughout.\nHands on Machine Learning with R (HOML, for short) by Bradley Boehmke and Brandon Greenwell is the textbook I used in the Machine Learning course I taught in spring of 2020. It is a great place to go to review some of the model algorthims and concepts.\nISLR by James, Witten, Hastie, and Tibshirani goes deeper into the math of the algorithms. You can download their book at this site.\nTidymodels\nLisa’s tidymodels noRth presentation gives an example of using tidymodels. I will go through it below.\ntidymodels.org, specifically the case study, walks through examples of using the tidymodels suite\nTidy Models with R textbook by Julia Silge and Max Kuhn provides more in-depth explanations of the tidymodels functions with extended examples.\nJulia Silge’s blog with even more examples!\n\nReview\nMost of you probably learned about machine learning algorithms using the caret R package. Before jumping into the new tidymodels package, let’s remember some of the key machine learning concepts.\nLet’s start with an overview of the process. You covered many of these in your machine learning course. If you need more of a refresher than what I provide, see the Modeling Process chapter of HOML.\n\nMachine Learning process\nAnd let’s review what we do during each of these steps.\nQuick exploration: Read in the data, check variable types, find which values each variable takes and how often, check distributions of quantitative variables, explore missing values. DO NOT do any modeling or transforming of data in this step.\nData splitting: Split the data into training and testing sets. The testing dataset will not be used again until the very end.\nData pre-processing: More in-depth data exploration, feature engineering, variable transformations. This step is usually pretty time-consuming.\nFitting model(s): Fit the models of interest on the training data.\nTuning parameters: If the model in the previous step involved tuning parameters, use cross-validation (or similar method) to find the best parameter.\nEvaluate & compare models: Use cross-validation to evaluate the model. If you have a large number of models you are evaluating, you will probably limit the set of models to your best/favorite few during this step. The image below is to help you remember that process, which I have also written about below.\n{width = 60% .external}\nIn \\(k\\)-fold cross-validation, we divide the data randomly into \\(k\\) approximately equal groups or folds. The schematic here shows 5-fold cross-validation.\nThe model is fit on \\(k-1\\) of the folds and the remaining fold is used to evaluate the model. Let’s look at the first row in the schematic. Here the model is fit on the data that are in folds 2, 3, 4, and 5. The model is evaluated on the data in fold 1.\nRMSE is a common performance metric for models with a quantitative response. It is computed by taking the difference between the predicted and actual response for each observation, squaring it, and taking the square root of the average over all observations. Or, as a formula:\n\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y}_i)^2},\n\\]\nSo, again looking at the first row in the schematic, the model is fit to folds 2, 3, 4, and 5 and we would use that model to compute the RMSE for fold 1. In the second row, the model is fit to the data in folds 1, 3, 4, and 5 and that model is used to compute the RMSE for the data in the 2nd fold.\nAfter this is done for all 5 folds, we take the average RMSE, to obtain the overall performance. This overall error is sometimes called the CV error. Averaging the performance over \\(k\\) folds gives a better estimate of the true error than using one hold-out set. It also allows us to estimate its variability.\nFor models with a categorical response, a common performance metric to evaluate a model is accuracy: out of all cases, fraction of correct (true positives and true negatives) classifications. A cross-validated accuracy would be computed in a similar way to the cross-validated RMSE described above.\nApply final few models to testing data: After we limit the number of models to the top few, we will want to to apply it to the testing data, the data that hasn’t been used at all during the modeling process. This will give us a measure of the model’s performance and may help us make a final decision about which model to use.\nUse the model!: This step may be simple, like applying the model to a single set of data, or it could be a lot more complex, requiring the model to be “put into production” so it can be applied to new data in real-time.\nQuestion the data and model: This isn’t really a single step but something that we should be doing through the modeling process. We should be working closely with people who know the data well so we assure that we are interpreting and using it correctly. And we should evaluate how the model might be used in new contexts, especially keeping in mind how the model could be used to do harm.\nUsing tidymodels for the process\nIn this section, I will show how we can use the tidymodels framework to execute the modeling process. I’ve updated the diagram from above to include some of the libraries and functions we’ll use throughout the process.\n\ntidymodels machine learning process\nFirst, let’s load some of the libraries we will use:\n\n\nlibrary(tidyverse)         # for reading in data, graphing, and cleaning\nlibrary(tidymodels)        # for modeling ... tidily\nlibrary(glmnet)            # for regularized regression, including LASSO\nlibrary(naniar)            # for examining missing values (NAs)\nlibrary(lubridate)         # for date manipulation\nlibrary(moderndive)        # for King County housing data\nlibrary(vip)               # for variable importance plots\ntheme_set(theme_minimal()) # my favorite ggplot2 theme :)\n\n\n\nRead in the King County Housing data and take a look at the first 5 rows.\n\n\ndata(\"house_prices\")\n\nhouse_prices %>% \n  slice(1:5)\n\n\n# A tibble: 5 x 21\n  id         date        price bedrooms bathrooms sqft_living sqft_lot\n  <chr>      <date>      <dbl>    <int>     <dbl>       <int>    <int>\n1 7129300520 2014-10-13 221900        3      1           1180     5650\n2 6414100192 2014-12-09 538000        3      2.25        2570     7242\n3 5631500400 2015-02-25 180000        2      1            770    10000\n4 2487200875 2014-12-09 604000        4      3           1960     5000\n5 1954400510 2015-02-18 510000        3      2           1680     8080\n# … with 14 more variables: floors <dbl>, waterfront <lgl>,\n#   view <int>, condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>\n\nNow, we will dig into each of the modeling steps listed above.\n1. Quick exploration\nTake a quick look at distributions of all the variables to check for anything irregular.\nQuantitative variables:\n\n\nhouse_prices %>% \n  select(where(is.numeric)) %>% \n  pivot_longer(cols = everything(),\n               names_to = \"variable\", \n               values_to = \"value\") %>% \n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(vars(variable), \n             scales = \"free\")\n\n\n\n\nThings I noticed and pre-processing thoughts: * Right-skewness in price and all variables regarding square footage –> log transform if using linear regression. * Many 0’s in sqft_basement, view, and yr_renovated –> create indicator variables of having that feature vs. not, ie. a variable called basement where a 0 indicates no basement (sqft_basement = 0) and a indicates a basement (sqft_basement> 0).   * Age of home may be a better, more interpretable variable than year built -->age_at_sale = year(date) - yr_built`.\nCategorical variables:\n\n\nhouse_prices %>% \n  select(where(is.factor)) %>% \n  pivot_longer(cols = everything(),\n               names_to = \"variable\", \n               values_to = \"value\") %>% \n  ggplot(aes(x = value)) +\n  geom_bar() +\n  facet_wrap(vars(variable), \n             scales = \"free\", \n             nrow = 2)\n\n\n\n\nThings I noticed and pre-processing thoughts:\ncondition and grade both have levels with low counts –> make fewer categories.\nzipcode has many unique levels –> don’t use that variable for now.\nWe might consider using the month the house was sold as a variable:\n\n\nhouse_prices %>% \n  count(month = month(date, label = TRUE)) %>% \n  ggplot() +\n  geom_col(aes(x = month, y = n))\n\n\n\n\nAnd, we quickly look at the counts for the waterfront variable. Not many houses are waterfront properties.\n\n\nhouse_prices %>% \n  count(waterfront)\n\n\n# A tibble: 2 x 2\n  waterfront     n\n* <lgl>      <int>\n1 FALSE      21450\n2 TRUE         163\n\nThe only other variable is id which isn’t used in modeling.\nBefore moving on, let’s use the add_n_miss() function from the naniar library to see if we have any missing values. And it appears that there aren’t any missing values - lucky us!\n\n\nhouse_prices %>% \n  add_n_miss() %>% \n  count(n_miss_all)\n\n\n# A tibble: 1 x 2\n  n_miss_all     n\n*      <int> <int>\n1          0 21613\n\n2. Data splitting\nNOTE: I start by doing some manipulating of the dataset to use log_price as the response variable rather than price. I originally did this using a step_log() function after a recipe() function (see the next section), but read in this RStudio Community post, in the comment by Max Kuhn, that it’s better to transform the outcome before doing the modeling. There is also a discussion of this in the Skipping steps for new data section of the Kuhn & Silge Tidy Modeling with R book.\nThen, we split the data into training and testing datasets. We use the training data to fit different types of models and to tune parameters of those models, if needed. The testing dataset is saved for the very end to compare a small subset of models. The initial_split() function from the rsample library (part of tidymodels) is used to create this split. We just do random splitting with this dataset, but there are other arguments that allow you to do stratified sampling. Then we use training() and testing() to extract the two datasets, house_training and house_testing.\n\n\nset.seed(327) #for reproducibility\n\nhouse_prices <- house_prices %>% \n  mutate(log_price = log(price, base = 10)) %>% \n  select(-price)\n\n# Randomly assigns 75% of the data to training.\nhouse_split <- initial_split(house_prices, \n                             prop = .75)\nhouse_split\n\n\n<Analysis/Assess/Total>\n<16210/5403/21613>\n\n#<training/testing/total>\n\nhouse_training <- training(house_split)\nhouse_testing <- testing(house_split)\n\n\n\n3. Data pre-processing\nThis step may not seem very time consuming in this example, but you will often come back to this step and spend a lot of time trying different variable transformations. You should make sure to work closely with the people who use and create the data during this step. They are a crucial part of the process.\nWe use the recipe() function to define the response/outcome variable and the predictor variables.\nA variety of step_xxx() functions can be used to do any data pre-processing/transforming. Find them all here. I used a few, with brief descriptions in the code. I also used some selector functions, like all_predictors() and all_nominal() to help me select the right variables.\nWe also use update_roles() to change the roles of some variables. For us, these are variables we may want to include for evaluation purposes but will not be used in building the model. I chose the role of evaluative but you could name that role anything you want, eg. id, extra, junk (maybe a bad idea?).\n\n\nhouse_recipe <- recipe(log_price ~ ., #short-cut, . = all other vars\n                       data = house_training) %>% \n  # Pre-processing:\n  # Remove, redundant to sqft_living and sqft_lot\n  step_rm(sqft_living15, sqft_lot15) %>%\n  # log sqft variables (without price)\n  step_log(starts_with(\"sqft\"),\n           -sqft_basement, \n           base = 10) %>% \n  # I originally had the step_log() function below\n  # but instead did the transformation before\n  # the recipe because this will mess up the \n  # predict() function\n  # step_log(price, base = 10) %>% \n  \n  # new grade variable combines low grades & high grades\n  # indicator variables for basement, renovate, and view \n  # waterfront to numeric\n  # age of house\n  step_mutate(grade = as.character(grade),\n              grade = fct_relevel(\n                        case_when(\n                          grade %in% \"1\":\"6\"   ~ \"below_average\",\n                          grade %in% \"10\":\"13\" ~ \"high\",\n                          TRUE ~ grade\n                        ),\n                        \"below_average\",\"7\",\"8\",\"9\",\"high\"),\n              basement = as.numeric(sqft_basement == 0),\n              renovated = as.numeric(yr_renovated == 0),\n              view = as.numeric(view == 0),\n              waterfront = as.numeric(waterfront),\n              age_at_sale = year(date) - yr_built)%>% \n  # Remove sqft_basement, yr_renovated, and yr_built\n  step_rm(sqft_basement, \n          yr_renovated, \n          yr_built) %>% \n  # Create a month variable\n  step_date(date, \n            features = \"month\") %>% \n  # Make these evaluative variables, not included in modeling\n  update_role(all_of(c(\"id\",\n                       \"date\",\n                       \"zipcode\", \n                       \"lat\", \n                       \"long\")),\n              new_role = \"evaluative\") %>% \n  # Create indicator variables for factors/character/nominal\n  step_dummy(all_nominal(), \n             all_predictors(), \n             -has_role(match = \"evaluative\"))\n\n\n\nApply to training dataset, just to see what happens. This is not a necessary step, but I often like to check to see that everything is as expected. For example, notice the names of the variables are the same as before but they have been transformed, eg. sqft_living is actually log base 10 of square feet of living. This confused me the first time, so I was glad I ran this extra step. Better to be confused now than later in the process 😀.\n\n\nhouse_recipe %>% \n  prep(house_training) %>%\n  # using bake(new_data = NULL) gives same result as juice()\n  # bake(new_data = NULL)\n  juice() \n\n\n# A tibble: 16,210 x 36\n   id        date       bedrooms bathrooms sqft_living sqft_lot floors\n   <fct>     <date>        <int>     <dbl>       <dbl>    <dbl>  <dbl>\n 1 71293005… 2014-10-13        3      1           3.07     3.75      1\n 2 64141001… 2014-12-09        3      2.25        3.41     3.86      2\n 3 56315004… 2015-02-25        2      1           2.89     4         1\n 4 24872008… 2014-12-09        4      3           3.29     3.70      1\n 5 19544005… 2015-02-18        3      2           3.23     3.91      1\n 6 72375503… 2014-05-12        4      4.5         3.73     5.01      1\n 7 13214000… 2014-06-27        3      2.25        3.23     3.83      2\n 8 20080002… 2015-01-15        3      1.5         3.03     3.99      1\n 9 24146001… 2015-04-15        3      1           3.25     3.87      1\n10 37935001… 2015-03-12        3      2.5         3.28     3.82      2\n# … with 16,200 more rows, and 29 more variables: waterfront <dbl>,\n#   view <dbl>, sqft_above <dbl>, zipcode <fct>, lat <dbl>,\n#   long <dbl>, log_price <dbl>, basement <dbl>, renovated <dbl>,\n#   age_at_sale <dbl>, condition_X2 <dbl>, condition_X3 <dbl>,\n#   condition_X4 <dbl>, condition_X5 <dbl>, grade_X7 <dbl>,\n#   grade_X8 <dbl>, grade_X9 <dbl>, grade_high <dbl>,\n#   date_month_Feb <dbl>, date_month_Mar <dbl>, date_month_Apr <dbl>,\n#   date_month_May <dbl>, date_month_Jun <dbl>, date_month_Jul <dbl>,\n#   date_month_Aug <dbl>, date_month_Sep <dbl>, date_month_Oct <dbl>,\n#   date_month_Nov <dbl>, date_month_Dec <dbl>\n\n4. Fitting model(s)\nNow that we have split and pre-processed the data, we are ready to model! First, we will model price (which is actually now log(price)) using simple linear regression.\nWe will do this using some modeling functions from the parsnip package. Find all available functions here. Here is the detail for linear regression.\nIn order to define our model, we need to do these steps:\nDefine the model type, which is the general type of model you want to fit.\nSet the engine, which defines the package/function that will be used to fit the model.\nSet the mode, which is either “regression” for continuous response variables or “classification” for binary/categorical response variables. (Note that for linear regression, it can only be “regression”, so we don’t NEED this step in this case.)\n(OPTIONAL) Set arguments to tune. We’ll see an example of this later.\n\n\nhouse_linear_mod <- \n  # Define a linear regression model\n  linear_reg() %>% \n  # Set the engine to \"lm\" (lm() function is used to fit model)\n  set_engine(\"lm\") %>% \n  # Not necessary here, but good to remember for other models\n  set_mode(\"regression\")\n\nhouse_linear_mod\n\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nThis is just setting up the process. We haven’t fit the model to data yet, and there’s still one more step before we do - creating a workflow! This combines the preprocessing and model definition steps.\n\n\nhouse_lm_wf <- \n  # Set up the workflow\n  workflow() %>% \n  # Add the recipe\n  add_recipe(house_recipe) %>% \n  # Add the modeling\n  add_model(house_linear_mod)\n\nhouse_lm_wf\n\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nNow we are finally ready to fit the model! After all that work, this part seems easy. We first use the fit() function to fit the model, telling it which data set we want to fit the model to. Then we use some other functions to display the results nicely.\n\n\nhouse_lm_fit <- \n  # Tell it the workflow\n  house_lm_wf %>% \n  # Fit the model to the training data\n  fit(house_training)\n\n# Display the results nicely\nhouse_lm_fit %>% \n  pull_workflow_fit() %>% \n  tidy() %>% \n  mutate(across(where(is.numeric), ~round(.x,3)))\n\n\n# A tibble: 31 x 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)    4.01      0.048     83.4        0\n 2 bedrooms      -0.018     0.002    -11.2        0\n 3 bathrooms      0.036     0.003     14.3        0\n 4 sqft_living    0.294     0.025     11.7        0\n 5 sqft_lot      -0.038     0.003    -11.0        0\n 6 floors         0.021     0.003      6.87       0\n 7 waterfront     0.194     0.013     15.0        0\n 8 view          -0.061     0.004    -15.6        0\n 9 sqft_above     0.149     0.025      5.99       0\n10 basement      -0.042     0.005     -9.14       0\n# … with 21 more rows\n\n6. Evaluate & compare models\n(I realize we skipped #5. Don’t worry, we’ll get to it.)\nTo evaluate the model, we will use cross-validation (CV), specifically 5-fold CV. First, we set up the five folds of the training data using the vfold_cv() function.\n\n\nset.seed(1211) # for reproducibility\nhouse_cv <- vfold_cv(house_training, v = 5)\n\n\n\nThen, we fit the model using the 5-fold dataset we just created (I am guessing we don’t have to do both the previous step of fitting a model on the training data AND this step, but I couldn’t figure out how to extract the final model from the CV data … so this was my solution for now … and it turns out you DO need to do both as noted by Julia Silge in this RStudio Community post).\n\n\nset.seed(456) # For reproducibility - not needed for this algorithm\n\nhouse_lm_fit_cv <-\n  # Tell it the workflow\n  house_lm_wf %>% \n  # Fit the model (using the workflow) to the cv data\n  fit_resamples(house_cv)\n\n# The evaluation metrics for each fold:\nhouse_lm_fit_cv %>% \n  select(id, .metrics) %>% \n  unnest(.metrics)\n\n\n# A tibble: 10 x 5\n   id    .metric .estimator .estimate .config             \n   <chr> <chr>   <chr>          <dbl> <chr>               \n 1 Fold1 rmse    standard       0.135 Preprocessor1_Model1\n 2 Fold1 rsq     standard       0.662 Preprocessor1_Model1\n 3 Fold2 rmse    standard       0.137 Preprocessor1_Model1\n 4 Fold2 rsq     standard       0.644 Preprocessor1_Model1\n 5 Fold3 rmse    standard       0.137 Preprocessor1_Model1\n 6 Fold3 rsq     standard       0.638 Preprocessor1_Model1\n 7 Fold4 rmse    standard       0.133 Preprocessor1_Model1\n 8 Fold4 rsq     standard       0.655 Preprocessor1_Model1\n 9 Fold5 rmse    standard       0.135 Preprocessor1_Model1\n10 Fold5 rsq     standard       0.642 Preprocessor1_Model1\n\n# Evaluation metrics averaged over all folds:\ncollect_metrics(house_lm_fit_cv)\n\n\n# A tibble: 2 x 6\n  .metric .estimator  mean     n  std_err .config             \n  <chr>   <chr>      <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   0.135     5 0.000668 Preprocessor1_Model1\n2 rsq     standard   0.648     5 0.00437  Preprocessor1_Model1\n\n# Just to show you where the averages come from:\nhouse_lm_fit_cv %>% \n  select(id, .metrics) %>% \n  unnest(.metrics) %>% \n  group_by(.metric, .estimator) %>% \n  summarize(mean = mean(.estimate),\n            n = n(),\n            std_err = sd(.estimate)/sqrt(n))\n\n\n# A tibble: 2 x 5\n# Groups:   .metric [2]\n  .metric .estimator  mean     n  std_err\n  <chr>   <chr>      <dbl> <int>    <dbl>\n1 rmse    standard   0.135     5 0.000668\n2 rsq     standard   0.648     5 0.00437 \n\n7. Apply model to testing data\nIn this simple scenario, we may be interested in seeing how the model performs on the testing data that was left out. The code below will fit the model to the training data and apply it to the testing data. There are other ways we could have done this, but the way we do it here will be useful when we start using more complex models where we need to tune model parameters.\nAfter the model is fit and applied, we collect the performance metrics and display them and show the predictions from the testing data.\n\n\nhouse_lm_test <- \n  # The modeling work flow\n  house_lm_wf %>% \n  # Use training data to fit the model and apply it to testing data\n  last_fit(house_split)\n\n# performance metrics from testing data\ncollect_metrics(house_lm_test)\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.135 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n\n# predictions from testing data\ncollect_predictions(house_lm_test)\n\n\n# A tibble: 5,403 x 5\n   id               .pred  .row log_price .config             \n   <chr>            <dbl> <int>     <dbl> <chr>               \n 1 train/test split  5.58    12      5.67 Preprocessor1_Model1\n 2 train/test split  5.53    17      5.60 Preprocessor1_Model1\n 3 train/test split  5.90    27      5.97 Preprocessor1_Model1\n 4 train/test split  5.58    29      5.64 Preprocessor1_Model1\n 5 train/test split  5.67    31      5.76 Preprocessor1_Model1\n 6 train/test split  5.88    38      5.81 Preprocessor1_Model1\n 7 train/test split  5.69    40      5.78 Preprocessor1_Model1\n 8 train/test split  5.79    41      5.80 Preprocessor1_Model1\n 9 train/test split  5.77    42      5.89 Preprocessor1_Model1\n10 train/test split  5.66    44      5.84 Preprocessor1_Model1\n# … with 5,393 more rows\n\nThe code below creates a simple plot to examine predicted vs. actual price (log base 10) from the house data.\n\n\ncollect_predictions(house_lm_test) %>% \n  ggplot(aes(x = log_price, \n             y = .pred)) +\n  geom_point(alpha = .5, \n             size = .5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, \n              intercept = 0, \n              color = \"darkred\") +\n  labs(x = \"Actual log(price)\", \n       y = \"Predicted log(price)\")\n\n\n\n\nHere is the same plot using the regular price scale.\n\n\ncollect_predictions(house_lm_test) %>% \n  ggplot(aes(x = 10^log_price, \n             y = 10^.pred)) +\n  geom_point(alpha = .5, \n             size = .5) +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, \n              intercept = 0, \n              color = \"darkred\") +\n  labs(x = \"Actual price\", \n       y = \"Predicted price\") +\n  scale_x_continuous(labels = scales::dollar_format(scale = .000001, \n                                                    suffix = \"M\")) +\n  scale_y_continuous(labels = scales::dollar_format(scale = .000001, \n                                                    suffix = \"M\"))\n\n\n\n\n9. Question the data and model\n(We’ll go back to step #8 in a moment)\nWhen we use create models, it is important to think about how the model will be used and specifically how the model could do harm. One thing to notice in the graphs above is that the price of lower priced homes are, on average, overestimated whereas the price of higher priced homes are, on average, underestimated.\nWhat if this model was used to determine the price of homes for property tax purposes? Then lower priced homes would be overtaxed while higher priced homes would be undertaxed.\nThere are many different ways we might continue to examine this model (eg. are there differences by zipcode) but for now, we’ll move on.\n8. Use the model\nHow might use this model? One simple way is to predict new values. We saw that we could add the predicted values to the test data using the collect_predictions() function. Below, I predict the value for one new observation using the predict() function. We put the values of each variable in a dataset, in this case a tibble(). We need to have values for all the variables that were originally in the dataset passed to the recipe(), even the evaluation ones that don’t get used in the model. We can have extra variables in there, though, like the one I have called garbage. I show a predicted value (for a linear model, type = \"numeric\") and a confidence interval (type = \"conf_int\").\nNOTE: This is a bit of an aside, but an important one. If I would have used the step_log() function to transform the response variable price in the pre-processing step, rather than transforming it before that, we would see an error message in the predict() below because it would try to run that transformation step, but there wouldn’t be a price variable. In real life, it would usually be the case that you don’t have a value for the variable you are trying to predict. I originally tried to solve this problem by adding skip = TRUE to the step_log() function, but then the evaluation metrics in collect_metrics() compared the predicted log price to the actual price - yikes! This is discussed in a few places online - here’s one. The solution is to transform the response variable before doing any of the modeling steps, as I mentioned in the Data splitting section.\n\n\npredict(\n  house_lm_fit,\n  new_data = tibble(id = \"0705700390\",\n                    date = ymd(\"2014-09-03\"),\n                    bedrooms = 3,\n                    bathrooms = 2.25,\n                    sqft_living = 2020,\n                    sqft_lot = 8379,\n                    floors = 2,\n                    waterfront = FALSE,\n                    view = 0,\n                    condition = \"3\",\n                    grade = \"7\",\n                    sqft_above = 2020,\n                    sqft_basement = 0,\n                    yr_built = 1994,\n                    yr_renovated = 0,\n                    zipcode = \"98038\",\n                    lat = 47.3828,\n                    long = -122.023,\n                    sqft_living15 = 2020,\n                    sqft_lot15 = 8031,\n                    garbage = \"look, it's garbage\"),\n  type = \"numeric\",\n  level = 0.95\n)\n\n\n# A tibble: 1 x 1\n  .pred\n  <dbl>\n1  5.55\n\npredict(\n  house_lm_fit,\n  new_data = tibble(id = \"0705700390\",\n                    date = ymd(\"2014-09-03\"),\n                    bedrooms = 3,\n                    bathrooms = 2.25,\n                    sqft_living = 2020,\n                    sqft_lot = 8379,\n                    floors = 2,\n                    waterfront = FALSE,\n                    view = 0,\n                    condition = \"3\",\n                    grade = \"7\",\n                    sqft_above = 2020,\n                    sqft_basement = 0,\n                    yr_built = 1994,\n                    yr_renovated = 0,\n                    zipcode = \"98038\",\n                    lat = 47.3828,\n                    long = -122.023,\n                    sqft_living15 = 2020,\n                    sqft_lot15 = 8031,\n                    garbage = \"look, it's garbage\"),\n  type = \"conf_int\",\n  level = 0.95\n)\n\n\n# A tibble: 1 x 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1        5.54        5.56\n\nWe could also give it an entire dataset. Here, I just take a sample from the original dataset, add an extra variable, and predict with it.\n\n\nset.seed(327)\n\nfake_new_data <- house_prices %>% \n  sample_n(20) %>% \n  mutate(extra_var = 1:20)\n\npredict(house_lm_fit, \n        fake_new_data)\n\n\n# A tibble: 20 x 1\n   .pred\n   <dbl>\n 1  5.55\n 2  5.50\n 3  5.61\n 4  5.36\n 5  5.54\n 6  5.59\n 7  5.56\n 8  5.65\n 9  5.34\n10  6.42\n11  5.68\n12  5.80\n13  5.98\n14  5.56\n15  5.50\n16  5.60\n17  5.73\n18  5.40\n19  5.96\n20  5.62\n\nSince the predict() function will always return the same number of rows and in the same order as the dataset we put in, we can easily append the prediction to the dataset.\n\n\nfake_new_data %>% \n  bind_cols(predict(house_lm_fit,\n                    fake_new_data))\n\n\n# A tibble: 20 x 23\n   id        date       bedrooms bathrooms sqft_living sqft_lot floors\n   <chr>     <date>        <int>     <dbl>       <int>    <int>  <dbl>\n 1 72023306… 2014-07-21        3      2.5         2020     5613    2  \n 2 66790010… 2014-12-18        3      2.5         1660     7388    2  \n 3 76822003… 2014-07-28        3      2.25        1960     8875    1  \n 4 61502004… 2014-05-13        2      0.75         650     5360    1  \n 5 86455113… 2014-12-01        3      1.75        1810    21138    1  \n 6 93210101… 2015-03-12        3      1.75        1390     8980    1  \n 7 78532708… 2014-08-05        3      2.5         2230     7934    2  \n 8 05100029… 2015-04-07        4      1           1640     4200    1.5\n 9 19016000… 2014-06-26        2      1            720     8040    1  \n10 12250690… 2014-05-05        7      8          13540   307752    3  \n11 11050007… 2015-01-23        3      1.5         1570    10824    2  \n12 36297601… 2014-08-21        3      2.5         2490     4904    2  \n13 75010000… 2014-11-21        4      3.5         3020    12750    2  \n14 86455400… 2014-07-25        3      2           1790     8228    1  \n15 21720002… 2015-03-23        2      1           1150    11250    1  \n16 79670002… 2014-11-21        3      2.5         1930     4000    2  \n17 40778002… 2014-06-10        3      1.5         2010     9480    1  \n18 04250000… 2014-10-21        2      1           1150     5695    1  \n19 95418001… 2014-10-10        5      2.5         3490    18850    1  \n20 43182004… 2014-05-22        3      2.25        1470     1578    2  \n# … with 16 more variables: waterfront <lgl>, view <int>,\n#   condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>, log_price <dbl>, extra_var <int>, .pred <dbl>\n\nWe could also add a confidence interval and use the relocate() function to move around some variables.\n\n\nfake_new_data %>% \n  bind_cols(predict(house_lm_fit,\n                    fake_new_data)) %>% \n  bind_cols(predict(house_lm_fit,\n                    fake_new_data, \n                    type = \"conf_int\")) %>% \n  relocate(log_price, starts_with(\".pred\"), \n           .after = id)\n\n\n# A tibble: 20 x 25\n   id      log_price .pred .pred_lower .pred_upper date       bedrooms\n   <chr>       <dbl> <dbl>       <dbl>       <dbl> <date>        <int>\n 1 720233…      5.72  5.55        5.54        5.56 2014-07-21        3\n 2 667900…      5.45  5.50        5.49        5.51 2014-12-18        3\n 3 768220…      5.26  5.61        5.61        5.62 2014-07-28        3\n 4 615020…      5.36  5.36        5.35        5.37 2014-05-13        2\n 5 864551…      5.48  5.54        5.53        5.55 2014-12-01        3\n 6 932101…      5.44  5.59        5.58        5.60 2015-03-12        3\n 7 785327…      5.65  5.56        5.55        5.57 2014-08-05        3\n 8 051000…      5.92  5.65        5.64        5.66 2015-04-07        4\n 9 190160…      5.32  5.34        5.33        5.35 2014-06-26        2\n10 122506…      6.36  6.42        6.40        6.44 2014-05-05        7\n11 110500…      5.36  5.68        5.67        5.69 2015-01-23        3\n12 362976…      5.80  5.80        5.79        5.81 2014-08-21        3\n13 750100…      5.93  5.98        5.97        5.99 2014-11-21        4\n14 864554…      5.50  5.56        5.56        5.57 2014-07-25        3\n15 217200…      5.41  5.50        5.49        5.51 2015-03-23        2\n16 796700…      5.54  5.60        5.59        5.61 2014-11-21        3\n17 407780…      5.63  5.73        5.72        5.74 2014-06-10        3\n18 042500…      5.26  5.40        5.39        5.41 2014-10-21        2\n19 954180…      5.96  5.96        5.95        5.97 2014-10-10        5\n20 431820…      5.64  5.62        5.61        5.63 2014-05-22        3\n# … with 18 more variables: bathrooms <dbl>, sqft_living <int>,\n#   sqft_lot <int>, floors <dbl>, waterfront <lgl>, view <int>,\n#   condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>, extra_var <int>\n\nWhat if we don’t want to use the model in this R session? That would be quite a common occurrence. After building a model, we would often want to save the model and use it at a later time to apply to new data. We’ll get into some more complex ways of doing this eventually, but for now, let’s do the following:\nSave the model using saveRDS(). This model is saved to the current project folder (assuming you’re using a project right now), but you could save it anywhere you’d like.\n\n\nsaveRDS(house_lm_fit, \"house_lm_fit.rds\")\n\n\n\nRead the model back in using readRDS(). Go look at house_lm_read in the environment, and you’ll see that, indeed, it is the workflow we saved! 🤗\n\n\nhouse_lm_read <- readRDS(\"house_lm_fit.rds\")\n\n\n\nUse the model we read back in to predict new data. Just like we did before, we can use the predict() function to predict new values. I use the same fake_new_data I used before.\n\n\nfake_new_data %>% \n  bind_cols(predict(house_lm_read,\n                    fake_new_data)) %>% \n  bind_cols(predict(house_lm_read,\n                    fake_new_data, \n                    type = \"conf_int\")) %>% \n  relocate(log_price, starts_with(\".pred\"), \n           .after = id)\n\n\n# A tibble: 20 x 25\n   id      log_price .pred .pred_lower .pred_upper date       bedrooms\n   <chr>       <dbl> <dbl>       <dbl>       <dbl> <date>        <int>\n 1 720233…      5.72  5.55        5.54        5.56 2014-07-21        3\n 2 667900…      5.45  5.50        5.49        5.51 2014-12-18        3\n 3 768220…      5.26  5.61        5.61        5.62 2014-07-28        3\n 4 615020…      5.36  5.36        5.35        5.37 2014-05-13        2\n 5 864551…      5.48  5.54        5.53        5.55 2014-12-01        3\n 6 932101…      5.44  5.59        5.58        5.60 2015-03-12        3\n 7 785327…      5.65  5.56        5.55        5.57 2014-08-05        3\n 8 051000…      5.92  5.65        5.64        5.66 2015-04-07        4\n 9 190160…      5.32  5.34        5.33        5.35 2014-06-26        2\n10 122506…      6.36  6.42        6.40        6.44 2014-05-05        7\n11 110500…      5.36  5.68        5.67        5.69 2015-01-23        3\n12 362976…      5.80  5.80        5.79        5.81 2014-08-21        3\n13 750100…      5.93  5.98        5.97        5.99 2014-11-21        4\n14 864554…      5.50  5.56        5.56        5.57 2014-07-25        3\n15 217200…      5.41  5.50        5.49        5.51 2015-03-23        2\n16 796700…      5.54  5.60        5.59        5.61 2014-11-21        3\n17 407780…      5.63  5.73        5.72        5.74 2014-06-10        3\n18 042500…      5.26  5.40        5.39        5.41 2014-10-21        2\n19 954180…      5.96  5.96        5.95        5.97 2014-10-10        5\n20 431820…      5.64  5.62        5.61        5.63 2014-05-22        3\n# … with 18 more variables: bathrooms <dbl>, sqft_living <int>,\n#   sqft_lot <int>, floors <dbl>, waterfront <lgl>, view <int>,\n#   condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>, extra_var <int>\n\n5. Tuning model parameters\nWith the first model, there weren’t any parameters to tune. Let’s go back to step #5 and look at how the workflow changes when we have to do this extra step.\nNow we are going to try using Least Absolute Shrinkage and Selection Operator (LASSO) regression. This method shrinks some coefficients to 0 based on a penalty term. We will use cross-validation to help us find the best penalty term.\nWe will set up the model similar to how we set up the linear model, but add a set_args() function. The tune() argument to the penalty term is a placeholder. We are telling it that we are going to tune the penalty parameter later.\n\n\nhouse_lasso_mod <- \n  # Define a lasso model \n  # I believe default is mixture = 1 so probably don't need \n  linear_reg(mixture = 1) %>% \n  # Set the engine to \"glmnet\" \n  set_engine(\"glmnet\") %>% \n  # The parameters we will tune.\n  set_args(penalty = tune()) %>% \n  # Use \"regression\"\n  set_mode(\"regression\")\n\n\n\nTo see the arguments available for tuning, go to the Explore Model Arguments section of the parsnip documentation and search the model type and engine you are interested in. Below I printed the arguments we can tune for linear_reg using glmnet (LASSO). We could have also tuned the mixture parameter, but I set it to 1 to explicitly use LASSO.\n\n{width = 60% .external}\n\nAnd then we create a LASSO workflow. Notice that we’re using the same recipe step that we used in the regular linear model.\n\n\nhouse_lasso_wf <- \n  # Set up the workflow\n  workflow() %>% \n  # Add the recipe\n  add_recipe(house_recipe) %>% \n  # Add the modeling\n  add_model(house_lasso_mod)\n\nhouse_lasso_wf\n\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n👀 Here’s where some of the new steps come in. We use the grid_regular() function from the dials library to choose some values of the penalty parameter for us. Alternatively, we could give it a vector of values we want to try.\n\n\npenalty_grid <- grid_regular(penalty(),\n                             levels = 20)\npenalty_grid\n\n\n# A tibble: 20 x 1\n    penalty\n      <dbl>\n 1 1.00e-10\n 2 3.36e-10\n 3 1.13e- 9\n 4 3.79e- 9\n 5 1.27e- 8\n 6 4.28e- 8\n 7 1.44e- 7\n 8 4.83e- 7\n 9 1.62e- 6\n10 5.46e- 6\n11 1.83e- 5\n12 6.16e- 5\n13 2.07e- 4\n14 6.95e- 4\n15 2.34e- 3\n16 7.85e- 3\n17 2.64e- 2\n18 8.86e- 2\n19 2.98e- 1\n20 1.00e+ 0\n\nThen, use the tune_grid() function to fit the model using cross-validation for all penalty_grid values and evaluate on all the folds.\n\n\nhouse_lasso_tune <- \n  house_lasso_wf %>% \n  tune_grid(\n    resamples = house_cv,\n    grid = penalty_grid\n    )\n\nhouse_lasso_tune\n\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 x 4\n  splits               id    .metrics          .notes          \n  <list>               <chr> <list>            <list>          \n1 <split [12968/3242]> Fold1 <tibble [40 × 5]> <tibble [2 × 1]>\n2 <split [12968/3242]> Fold2 <tibble [40 × 5]> <tibble [2 × 1]>\n3 <split [12968/3242]> Fold3 <tibble [40 × 5]> <tibble [2 × 1]>\n4 <split [12968/3242]> Fold4 <tibble [40 × 5]> <tibble [2 × 1]>\n5 <split [12968/3242]> Fold5 <tibble [40 × 5]> <tibble [2 × 1]>\n\nThen look at the cross-validated results in a table.\n\n\n# The rmse for each fold:\nhouse_lasso_tune %>% \n  select(id, .metrics) %>% \n  unnest(.metrics) %>% \n  filter(.metric == \"rmse\")\n\n\n# A tibble: 100 x 6\n   id     penalty .metric .estimator .estimate .config              \n   <chr>    <dbl> <chr>   <chr>          <dbl> <chr>                \n 1 Fold1 1.00e-10 rmse    standard       0.135 Preprocessor1_Model01\n 2 Fold1 3.36e-10 rmse    standard       0.135 Preprocessor1_Model02\n 3 Fold1 1.13e- 9 rmse    standard       0.135 Preprocessor1_Model03\n 4 Fold1 3.79e- 9 rmse    standard       0.135 Preprocessor1_Model04\n 5 Fold1 1.27e- 8 rmse    standard       0.135 Preprocessor1_Model05\n 6 Fold1 4.28e- 8 rmse    standard       0.135 Preprocessor1_Model06\n 7 Fold1 1.44e- 7 rmse    standard       0.135 Preprocessor1_Model07\n 8 Fold1 4.83e- 7 rmse    standard       0.135 Preprocessor1_Model08\n 9 Fold1 1.62e- 6 rmse    standard       0.135 Preprocessor1_Model09\n10 Fold1 5.46e- 6 rmse    standard       0.135 Preprocessor1_Model10\n# … with 90 more rows\n\n# rmse averaged over all folds:\nhouse_lasso_tune %>% \n  collect_metrics() %>% \n  filter(.metric == \"rmse\")\n\n\n# A tibble: 20 x 7\n    penalty .metric .estimator  mean     n  std_err .config           \n      <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>             \n 1 1.00e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 2 3.36e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 3 1.13e- 9 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 4 3.79e- 9 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 5 1.27e- 8 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 6 4.28e- 8 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 7 1.44e- 7 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 8 4.83e- 7 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 9 1.62e- 6 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n10 5.46e- 6 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n11 1.83e- 5 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n12 6.16e- 5 rmse    standard   0.135     5 0.000640 Preprocessor1_Mod…\n13 2.07e- 4 rmse    standard   0.135     5 0.000623 Preprocessor1_Mod…\n14 6.95e- 4 rmse    standard   0.135     5 0.000591 Preprocessor1_Mod…\n15 2.34e- 3 rmse    standard   0.136     5 0.000522 Preprocessor1_Mod…\n16 7.85e- 3 rmse    standard   0.142     5 0.000513 Preprocessor1_Mod…\n17 2.64e- 2 rmse    standard   0.161     5 0.000515 Preprocessor1_Mod…\n18 8.86e- 2 rmse    standard   0.191     5 0.000896 Preprocessor1_Mod…\n19 2.98e- 1 rmse    standard   0.228     5 0.000960 Preprocessor1_Mod…\n20 1.00e+ 0 rmse    standard   0.228     5 0.000960 Preprocessor1_Mod…\n\nAnd, even better, we can visualize the results. We can see that the RMSE stays fairly consistently low until just before \\(10^{-3}\\)\n\n\n# Visualize rmse vs. penalty\nhouse_lasso_tune %>% \n  collect_metrics() %>% \n  filter(.metric == \"rmse\") %>% \n  ggplot(aes(x = penalty, y = mean)) +\n  geom_point() +\n  geom_line() +\n  scale_x_log10(\n   breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n   labels = scales::trans_format(\"log10\",scales::math_format(10^.x))) +\n  labs(x = \"penalty\", y = \"rmse\")\n\n\n\n\nWe choose the best penalty parameter as the one with the smallest cross-validated RMSE. The select_best() function does this.\n\n\nhouse_lasso_tune %>% \n  show_best(metric = \"rmse\")\n\n\n# A tibble: 5 x 7\n   penalty .metric .estimator  mean     n  std_err .config            \n     <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>              \n1 2.07e- 4 rmse    standard   0.135     5 0.000623 Preprocessor1_Mode…\n2 6.16e- 5 rmse    standard   0.135     5 0.000640 Preprocessor1_Mode…\n3 1.00e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mode…\n4 3.36e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mode…\n5 1.13e- 9 rmse    standard   0.135     5 0.000644 Preprocessor1_Mode…\n\n\n\n# Best tuning parameter by smallest rmse\nbest_param <- house_lasso_tune %>% \n  select_best(metric = \"rmse\")\nbest_param\n\n\n# A tibble: 1 x 2\n   penalty .config              \n     <dbl> <chr>                \n1 0.000207 Preprocessor1_Model13\n\nThere are other ways you can select parameters, like select_by_one_std_err() which “selects the most simple model that is within one standard error of the numerically optimal results”. To use this, we need at least one more argument: the parameter to sort the model from most simple to most complex. So, if using glmnet’s penalty parameter, since a bigger penalty will be a simpler model, I should put desc(penalty) in as the argument.\n\n\n# Best tuning parameter by smallest rmse\none_se_param <- house_lasso_tune %>% \n  select_by_one_std_err(metric = \"rmse\", desc(penalty))\none_se_param\n\n\n# A tibble: 1 x 9\n   penalty .metric .estimator  mean     n std_err .config .best .bound\n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>   <dbl>  <dbl>\n1 0.000695 rmse    standard   0.135     5 5.91e-4 Prepro… 0.135  0.136\n\nBecause a larger penalty parameter will fit a simpler model (more terms will likely go to zero). I’ll go with the one_se_param, especially since the RMSE is so close to the “best” model’s RMSE.\nOnce we choose the parameter we want, we adjust the workflow to include the best tuning parameter using the finalize_workflow() function.\n\n\nhouse_lasso_final_wf <- house_lasso_wf %>% \n  finalize_workflow(one_se_param)\nhouse_lasso_final_wf\n\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.00069519279617756\n  mixture = 1\n\nComputational engine: glmnet \n\nNow we could fit this to the training data and look at the resulting model. We can see a few of the terms have coefficients of 0 (although not as many as I would have expected).\n\n\nhouse_lasso_final_mod <- house_lasso_final_wf %>% \n  fit(data = house_training)\n\nhouse_lasso_final_mod %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n\n# A tibble: 31 x 3\n   term        estimate  penalty\n   <chr>          <dbl>    <dbl>\n 1 (Intercept)   4.12   0.000695\n 2 bedrooms     -0.0160 0.000695\n 3 bathrooms     0.0348 0.000695\n 4 sqft_living   0.326  0.000695\n 5 sqft_lot     -0.0366 0.000695\n 6 floors        0.0209 0.000695\n 7 waterfront    0.186  0.000695\n 8 view         -0.0617 0.000695\n 9 sqft_above    0.124  0.000695\n10 basement     -0.0379 0.000695\n# … with 21 more rows\n\nWe can also visualize variable importance.\n\n\n# Visualize variable importance\nhouse_lasso_final_mod %>% \n  pull_workflow_fit() %>% \n  vip()\n\n\n\n\nLastly, we apply the model to the test data and examine some final metrics. We also show the metrics from the regular linear model. It looks like performance for the LASSO model is ever so slightly better, but just barely. It’s also a good sign that these RMSE’s are similar to the cross-validated RMSE’s.\n\n\n# Fit model with best tuning parameter(s) to training data and apply to test data\nhouse_lasso_test <- house_lasso_final_wf %>% \n  last_fit(house_split)\n\n# Metrics for model applied to test data\nhouse_lasso_test %>% \n  collect_metrics()\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.135 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n\n# Compare to regular linear regression results\ncollect_metrics(house_lm_test)\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.135 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n\n\n\n\n",
    "preview": "posts/2021-03-16-ml-review/ml-review_files/figure-html5/expl_quant-1.png",
    "last_modified": "2021-03-11T16:33:42-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-28-gitgithub/",
    "title": "Using git and GitHub in R Studio",
    "description": {},
    "author": [],
    "date": "2021-02-25",
    "categories": [],
    "contents": "\nQuick Intro\nIf you haven’t used Git and Github before, you should read and work through the github for collaboration instructions through the Your turn!! section right before the Adding collaborators section. If you are not able to get things working fairly quickly (~ 1 hour, not including installation times), then contact me so I can help you.\nIf you have used Git and Github before but NOT with R Studio, then you can probably start at Step 5 in the Install Git section. If you have used them in R Studio before, try going right to the Create your first repo and use it with R Studio section and ensure that everything still works.\nFor this class, you should plan to always start a new assignment/project by creating a github repo. And, in R Studio you should open the .Rproj file when you want to work with files within the project (see a short video of what I mean here). Your R project is a folder on your computer and is very much connected with the repo on github. For\nAt first, I want you to get used to using GitHub regularly for your own files. You should be comfortable with creating a repository on GitHub, cloning it in R Studio, and then consistently following the save –> commit –> push pattern. For a deeper understanding of some of this, see Jenny Bryan’s fantastic Happy Git with R resource.\nUsing Git and GitHub for collaboration\nThere are some beginning instructions in the github for collaboration documentation starting at the Adding collaborators section. For a slightly more in-depth coverage, including creating branches, read my blog post that covers that.\nIt is VERY important to remember that communication is a key component to using GitHub successfully for collaboration. Don’t forget that part!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-25T14:17:56-06:00",
    "input_file": {}
  }
]
