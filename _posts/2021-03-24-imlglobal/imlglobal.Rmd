---
title: "Interpretable Machine Learning"
description: |
  This tutorial focuses on global interpretation.
output:
  distill::distill_article:
    toc: true
    toc_float: true
    self_contained: false
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      error = TRUE)
```

```{r echo=FALSE}
library(emo)          # for emojis!   
library(downloadthis) # for including download buttons for files
```

```{r paged-table, echo=FALSE}
# define a method for objects of the class data.frame
# see https://github.com/rstudio/distill/issues/310#issuecomment-797541459
library(knitr)
knit_print.data.frame <- function(x, ...) {
  asis_output(
    rmarkdown:::paged_table_html(x, options = attr(x, "options")),
    meta = list(dependencies = rmarkdown:::html_dependency_pagedtable())
  )
}
registerS3method("knit_print", "data.frame", knit_print.data.frame)
```

## Follow along

You can download this .Rmd file below if you'd like to follow along. I do have a few hidden notes you can disregard. This document is a distill_article, so you may want to change to an html_document to knit. You will also need to delete any image references to properly knit, since you won't have those images.

```{r, echo=FALSE}
download_file(
  path = "unkglobal.Rmd",
  button_label = "Download .Rmd file",
  button_type = "info",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```

## Resources

* [Interpretable Machine Learning](https://bradleyboehmke.github.io/HOML/iml.html) chapter of HOML by by Bradley Boehmke & Brandon Greenwell. 

* [Explanatory Model Analysis](https://ema.drwhy.ai/modelLevelExploration.html) by Przemyslaw Biecek and Tomasz Burzykowski, section III: Dataset Level chapters.

## Set up

First, we load the libraries we will use. There will be some new ones you'll need to install.

```{r libraries}
library(tidyverse)         # for reading in data, graphing, and cleaning
library(tidymodels)        # for modeling ... tidily
library(stacks)            # for stacking models
library(moderndive)        # for King County housing data
library(vip)               # for variable importance plots
library(DALEX)             # moDel Agnostic Language for Exploration and eXplanation (for model interpretation)  
library(DALEXtra)          # for extension of DALEX
library(patchwork)         # for combining plots nicely
library(rmarkdown)         # for paged tables
theme_set(theme_minimal()) # my favorite ggplot2 theme :)
```

Then we load the data we will use throughout this tutorial and do some modifications.
```{r}
# SEE modeldata package for new datasets
data("house_prices")

# Create log_price and drop price variable
house_prices <- house_prices %>% 
  mutate(log_price = log(price, base = 10)) %>% 
  select(-price)

```


## Intro

In your machine learning course, you probably spent some time discussing pros and cons of the different types of model or algorithms. If you think way back to Intro to Statistical Modeling, you probably remember spending A LOT of time interpreting linear and logistic regression models. That is one huge advantage to those models: we can easily state the relationship between the predictors and the response variables just using the coefficients of the model. Even when we have somewhat complex models that include things like interaction terms, there is often a fairly easy interpretation. 

As we use more complex algorithms like random forests, gradient boosted machines, and deep learning, explaining the model becomes quite tricky. But it is important to try to understand. This is especially true in cases where the model is being applied to or affecting people. 

We will learn ways of interpreting our models globally and locally. This tutorial focuses on global model interpretations, where we try to understand the overall relationships between the predictor variables and the response. Next week, we'll learn about local model interpretations where we try to understand the impact variables have on individual observations. 

## Build some models

Let's use the King County house price data again. We will build a lasso model, like in the [Intro to tidymodels tutorial](https://advanced-ds-in-r.netlify.app/posts/2021-03-16-ml-review/#using-tidymodels-for-the-process) and the random forest model from the [stacking tutorial](https://advanced-ds-in-r.netlify.app/posts/2021-03-22-stacking/#this-isnt-so-new). We wouldn't have to use `log_price`, but I'm going to keep it that way so I can reference some of the output from that model. 

Recreate the lasso model (I used `select_best()` rather than `select_one_std_err()` like I did originally): 

```{r lasso-mod, cache=TRUE}
set.seed(327) #for reproducibility

# Randomly assigns 75% of the data to training.
house_split <- initial_split(house_prices, 
                             prop = .75)
house_training <- training(house_split)
house_testing <- testing(house_split)

# lasso recipe and transformation steps
house_recipe <- recipe(log_price ~ ., 
                       data = house_training) %>% 
  step_rm(sqft_living15, sqft_lot15) %>%
  step_log(starts_with("sqft"),
           -sqft_basement, 
           base = 10) %>% 
  step_mutate(grade = as.character(grade),
              grade = fct_relevel(
                        case_when(
                          grade %in% "1":"6"   ~ "below_average",
                          grade %in% "10":"13" ~ "high",
                          TRUE ~ grade
                        ),
                        "below_average","7","8","9","high"),
              basement = as.numeric(sqft_basement == 0),
              renovated = as.numeric(yr_renovated == 0),
              view = as.numeric(view == 0),
              waterfront = as.numeric(waterfront),
              age_at_sale = year(date) - yr_built)%>% 
  step_rm(sqft_basement, 
          yr_renovated, 
          yr_built) %>% 
  step_date(date, 
            features = "month") %>% 
  update_role(all_of(c("id",
                       "date",
                       "zipcode", 
                       "lat", 
                       "long")),
              new_role = "evaluative") %>% 
  step_dummy(all_nominal(), 
             -all_outcomes(), 
             -has_role(match = "evaluative")) %>% 
  step_normalize(all_predictors(), 
                 -all_nominal())

#define lasso model
house_lasso_mod <- 
  linear_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("regression")

# create workflow
house_lasso_wf <- 
  workflow() %>% 
  add_recipe(house_recipe) %>% 
  add_model(house_lasso_mod)

# create cv samples
set.seed(1211) # for reproducibility
house_cv <- vfold_cv(house_training, v = 5)


# penalty grid - changed to 10 levels
penalty_grid <- grid_regular(penalty(),
                             levels = 10)

# tune the model 
house_lasso_tune <- 
  house_lasso_wf %>% 
  tune_grid(
    resamples = house_cv,
    grid = penalty_grid
    )

# choose the best penalty
best_param <- house_lasso_tune %>% 
  select_best(metric = "rmse")

# finalize workflow
house_lasso_final_wf <- house_lasso_wf %>% 
  finalize_workflow(best_param)

# fit final model
house_lasso_final_mod <- house_lasso_final_wf %>% 
  fit(data = house_training)

# compute the training rmse - we'll compare to this later
house_training %>% 
  select(log_price) %>% 
  bind_cols(
    predict(house_lasso_final_mod, 
            new_data = house_training)
    ) %>%
  summarize(
    training_rmse = sqrt(mean((log_price - .pred)^2))
    )
```

Recreate the random forest model:

```{r rf-model, cache=TRUE}
# set up recipe and transformation steps and roles
ranger_recipe <- 
  recipe(formula = log_price ~ ., 
         data = house_training) %>% 
  step_date(date, 
            features = "month") %>% 
  # Make these evaluative variables, not included in modeling
  update_role(all_of(c("id",
                       "date")),
              new_role = "evaluative")

#define model
ranger_spec <- 
  rand_forest(mtry = 6, 
              min_n = 10, 
              trees = 200) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")

#create workflow
ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

#fit the model
set.seed(712) # for reproducibility - random sampling in random forest choosing number of variables
ranger_fit <- ranger_workflow %>% 
  fit(house_training)

# compute the training rmse - we'll compare to this later
house_training %>% 
  select(log_price) %>% 
  bind_cols(predict(ranger_fit, new_data = house_training)) %>%
  summarize(training_rmse = sqrt(mean((log_price - .pred)^2)))
```

## Global Model Interpretation

We will be using some functions from the `DALEX` and `DALEXtra` packages for model explanation. The first step is to create an "explainer". According the [documentation](https://ema.drwhy.ai/dataSetsIntro.html#ExplainersTitanicRCode), this is "an object that provides a uniform interface for different models." There is a generic `explain()` function, but we will use the `explain_tidymodels()` function which is set up to work well with models built with the `tidymodels` framework. We need to provide the `model`; `data`, which is the dataset we will be using for interpretation WITHOUT the outcome variable; and `y` which is a vector of the outcome variable that corresponds to the same observations from the `data` argument. We will also provide a `label`, an optional argument to identify our models.

(NOTE: I used the training data. I'm not positive which data should be used, to be honest, but most of the examples I saw used the training data, so that's what I went with. Christoph Molnar discusses this in-depth regarding [Variable Importance](https://christophm.github.io/interpretable-ml-book/feature-importance.html#feature-importance-data).)

Create explainer for the lasso model:

```{r}
lasso_explain <- 
  explain_tidymodels(
    model = house_lasso_final_mod,
    data = house_training %>% select(-log_price), 
    y = house_training %>%  pull(log_price),
    label = "lasso"
  )
```

Create explainer for the random forest model:

```{r}
rf_explain <- 
  explain_tidymodels(
    model = ranger_fit,
    data = house_training %>% select(-log_price), 
    y = house_training %>%  pull(log_price),
    label = "rf"
  )
```

### Model performance

One way to explain a model is through its performance, using statistics like RMSE for models involving a quantitative response variable or accuracy or AUC for models with a binary response variable. We already know how to look at these statistics, but we'll see how to use some functions from `DALEX` and `DALEXtra` to do it. 

The `model_performance()` function gives some overall model evaluation metrics. We are using the training data right now, so these metrics will be different from the cross-validated or test data results. They do match the training rmse I computed in the previous section. 

```{r}
lasso_mod_perf <- model_performance(lasso_explain)
rf_mod_perf <-  model_performance(rf_explain)

# lasso model performance
lasso_mod_perf

# random forest model performance
rf_mod_perf
```

We can plot the results using the `plot()` function. See `?plot.model_performance` for info on the different `geom` options (you may need to adjust the `fig.width` and `fig.height` in the R code chunk options to make this look nice). For example, with binary outcome models, you may be more interested in an ROC curve (`geom = "roc"`). 

Below we plot both histograms of residuals and boxplots of the absolute residuals. Note that the histogram labels are reversed, but the colors match the boxplots correctly. I have filed an [issue](https://github.com/ModelOriented/DALEX/issues/400). 

```{r, fig.width=7, fig.height=4}
hist_plot <- 
  plot(lasso_mod_perf,
       rf_mod_perf, 
       geom = "histogram")
box_plot <-
  plot(lasso_mod_perf,
       rf_mod_perf, 
       geom = "boxplot")

hist_plot + box_plot
```

### Variable importance

You have already seen at least one variable importance plot, but let's dig in a bit more into how they are constructed. Boehmke and Greenwell give an excellent, simple explanation in [section 16.3.1](https://bradleyboehmke.github.io/HOML/iml.html#concept) of their Hands on Machine Learning in R textbook. I will give a slightly modified version below.

1. Compute the performance metric for the model (RMSE, accuracy, etc.).  
2. For each variable in the model, do the following:
  * Permute the values. This means you randomly mix up the values for that variable across observations. For example, if we have ten observations of a variable with the values (2,3,4,3,2,5,6,7,8,5), a possible permutation would be (5,2,8,3,4,6,2,3,5,7). 
  * Fit the model again with this new permuted variable in place of the original variable.  
  * Compute the performance metric of interest (RMSE, accuracy, etc.).  
  * Find the variable's importance by comparing the new performance metric to the original model's performance metric (difference or ratio, usually scaled)  
3. Sort variables by descending importance

This procedure could be repeated multiple times to obtain an estimate of the variability of importance.

Let's consider a model with a quantitative response. If permuting variable `x` greatly increases the RMSE relative to permuting other variables, then variable `x` would be important. With a binary response, permuted variables that greatly decrease the accuracy relative to other variables would be important. Variable importance is a nice, easy interpretation. It tells us how much the model error would increase if that variable weren't in the model. 

Now, let's use some functions from `DALEX` to help us compute and graph variable importance. First, we use the `model_parts()` function. The only argument we need to provide is an explainer. By default, it will use RMSE for regression and 1-AUC for classification for the `loss_function` - it is possible to write your own.  The `type` argument tells it whether to use the raw loss (default), difference (`type = difference`) between permuted and original, or ratio (`type = ratio`) between the permuted and original. The `N` argument tells it how many observations to sample to do the computation. The default is 1000. 

Create a variable (aka feature) importance plot for the lasso model: 

```{r vip, fig.width=5, fig.height=3}
set.seed(10) #since we are sampling & permuting, we set a seed so we can replicate the results
lasso_var_imp <- 
  model_parts(
    lasso_explain
    )

plot(lasso_var_imp, show_boxplots = TRUE)
```

