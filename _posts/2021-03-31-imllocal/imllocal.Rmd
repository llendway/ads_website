---
title: "Interpretable Machine Learning"
description: |
  This tutorial focuses on local interpretation.
output:
  distill::distill_article:
    toc: true
    toc_float: true
    self_contained: false
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      error = TRUE)
```

```{r echo=FALSE}
library(emo)          # for emojis!   
library(downloadthis) # for including download buttons for files
```

```{r paged-table, echo=FALSE}
# define a method for objects of the class data.frame
# see https://github.com/rstudio/distill/issues/310#issuecomment-797541459
library(knitr)
knit_print.data.frame <- function(x, ...) {
  asis_output(
    rmarkdown:::paged_table_html(x, options = attr(x, "options")),
    meta = list(dependencies = rmarkdown:::html_dependency_pagedtable())
  )
}
registerS3method("knit_print", "data.frame", knit_print.data.frame)
```

## Follow along

You can download this .Rmd file below if you'd like to follow along. I do have a few hidden notes you can disregard. This document is a distill_article, so you may want to change to an html_document to knit. You will also need to delete any image references to properly knit, since you won't have those images.

```{r, echo=FALSE}
download_file(
  path = "imllocal.Rmd",
  button_label = "Download .Rmd file",
  button_type = "info",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```

## Resources

* [Explanatory Model Analysis](https://ema.drwhy.ai/InstanceLevelExploration.html) by Przemyslaw Biecek and Tomasz Burzykowski, section II: Instance Level chapters.

* [Interpretable Machine Learning](https://bradleyboehmke.github.io/HOML/iml.html) chapter of HOML by by Bradley Boehmke & Brandon Greenwell. 

* [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/agnostic.html) book by Christoph Molnar, specifically chapter 5.


## Set up

First, we load the libraries we will use. There will be some new ones you'll need to install.

```{r libraries}
library(tidyverse)         # for reading in data, graphing, and cleaning
library(tidymodels)        # for modeling ... tidily
library(lubridate)         # for dates
library(moderndive)        # for King County housing data
library(DALEX)             # moDel Agnostic Language for Exploration and eXplanation (for model interpretation)  
library(DALEXtra)          # for extension of DALEX
library(patchwork)         # for combining plots nicely
library(rmarkdown)         # for paged tables
theme_set(theme_minimal()) # my favorite ggplot2 theme :)
```

Then we load the data we will use throughout this tutorial and do some modifications.

```{r}
data("house_prices")

# Create log_price and drop price variable
house_prices <- house_prices %>% 
  mutate(log_price = log(price, base = 10)) %>% 
  # make all integers numeric ... fixes prediction problem
  mutate(across(where(is.integer), as.numeric)) %>% 
  select(-price)
```

## Intro

As mentioned in the global model interpretation tutorial last week, local model interpretation helps us understand the impact variables have on individual observations. In this tutorial, I will give a general overview of a few methods and show you some R code that can help you execute them. I highly suggest reading the resources I listed above, especially [Explanatory Model Analysis](https://ema.drwhy.ai/InstanceLevelExploration.html), which is a companion to the `DALEX` package we will be using.


## Recreate some models

Once again we will build the lasso and random forest models to predict `log_price` of a house from the King County data. As mentioned in the previous tutorial, we wouldn't have to use `log_price` for random forest, but I'm going to keep it that way so it's the same as the lasso model. 

Recreate the lasso model: 

```{r lasso-mod}
set.seed(327) #for reproducibility

# Randomly assigns 75% of the data to training.
house_split <- initial_split(house_prices, 
                             prop = .75)
house_training <- training(house_split)
house_testing <- testing(house_split)

# lasso recipe and transformation steps
house_recipe <- recipe(log_price ~ ., 
                       data = house_training) %>% 
  step_rm(sqft_living15, sqft_lot15) %>%
  step_log(starts_with("sqft"),
           -sqft_basement, 
           base = 10) %>% 
  step_mutate(grade = as.character(grade),
              grade = fct_relevel(
                        case_when(
                          grade %in% "1":"6"   ~ "below_average",
                          grade %in% "10":"13" ~ "high",
                          TRUE ~ grade
                        ),
                        "below_average","7","8","9","high"),
              basement = as.numeric(sqft_basement == 0),
              renovated = as.numeric(yr_renovated == 0),
              view = as.numeric(view == 0),
              waterfront = as.numeric(waterfront),
              age_at_sale = year(date) - yr_built)%>% 
  step_rm(sqft_basement, 
          yr_renovated, 
          yr_built) %>% 
  step_date(date, 
            features = "month") %>% 
  update_role(all_of(c("id",
                       "date",
                       "zipcode", 
                       "lat", 
                       "long")),
              new_role = "evaluative") %>% 
  step_dummy(all_nominal(), 
             -all_outcomes(), 
             -has_role(match = "evaluative")) %>% 
  step_normalize(all_predictors(), 
                 -all_nominal())

#define lasso model
house_lasso_mod <- 
  linear_reg(mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_args(penalty = tune()) %>% 
  set_mode("regression")

# create workflow
house_lasso_wf <- 
  workflow() %>% 
  add_recipe(house_recipe) %>% 
  add_model(house_lasso_mod)

# create cv samples
set.seed(1211) # for reproducibility
house_cv <- vfold_cv(house_training, v = 5)


# penalty grid - changed to 10 levels
penalty_grid <- grid_regular(penalty(),
                             levels = 10)

# tune the model 
house_lasso_tune <- 
  house_lasso_wf %>% 
  tune_grid(
    resamples = house_cv,
    grid = penalty_grid
    )

# choose the best penalty
best_param <- house_lasso_tune %>% 
  select_best(metric = "rmse")

# finalize workflow
house_lasso_final_wf <- house_lasso_wf %>% 
  finalize_workflow(best_param)

# fit final model
house_lasso_final_mod <- house_lasso_final_wf %>% 
  fit(data = house_training)
```

Recreate the random forest model:

```{r rf-model}
# set up recipe and transformation steps and roles
ranger_recipe <- 
  recipe(formula = log_price ~ ., 
         data = house_training) %>% 
  step_date(date, 
            features = "month") %>% 
  # Make these evaluative variables, not included in modeling
  update_role(all_of(c("id",
                       "date")),
              new_role = "evaluative")

#define model
ranger_spec <- 
  rand_forest(mtry = 6, 
              min_n = 10, 
              trees = 200) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")

#create workflow
ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

#fit the model
set.seed(712) # for reproducibility - random sampling in random forest choosing number of variables
ranger_fit <- ranger_workflow %>% 
  fit(house_training)
```

## Individual explanation


### CP profiles

Link to prior material

### Break down plots

Estimate the contribution of each variable. If model isn't additive, can change depending on order of variables.

Let's look at one and then explaing. First, explainers.

```{r}
lasso_explain <- 
  explain_tidymodels(
    model = house_lasso_final_mod,
    data = house_training %>% select(-log_price), 
    y = house_training %>%  pull(log_price),
    label = "lasso"
  )

rf_explain <- 
  explain_tidymodels(
    model = ranger_fit,
    data = house_training %>% select(-log_price), 
    y = house_training %>%  pull(log_price),
    label = "rf"
  )
```

Now create the plot. 

```{r}
#Choose an expensive house
new_obs <- house_testing %>% slice(5377) 

#price - just to know because I can't think in logs
10^(new_obs$log_price)

pp_lasso <- predict_parts(explainer = lasso_explain,
                          new_observation = new_obs,
                          type = "break_down") #default

plot(pp_lasso)

pp_lasso

pp_lasso %>% 
  as_tibble()
```

What does this show?

Look at intercept bar. It is the average predicted price (*predicted*, not actual price) for all the training data (fed into the explainer). We could compute that "by hand".

```{r}
mean(rf_explain$y_hat)

predict()

predict(ranger_fit, new_data = new_obs %>% mutate(grade = "10"))
```

Then, the `+.254` for the `grade = 10`  bar, is the average prediction if grade was fixed at 10. We could also do that "by hand".

```{r}
data_grade10 <- rf_explain$data %>% 
  mutate(grade = "11")

predict(ranger_fit, new_data = data_grade10) %>% 
  summarize(avg_grade10 = mean(.pred)) %>% 
  pull(avg_grade10)

colMeans(predict(ranger_fit, new_data = data_grade10))
```


```{r}
library("randomForest")
set.seed(1313)
titanic_rf <- randomForest(survived ~ class + gender + age + 
                sibsp + parch + fare + embarked, 
                data = titanic %>% drop_na())

explain_titanic <- DALEX::explain(model = titanic_rf,  
                        data = titanic %>% drop_na() %>% select(-survived),
                           y = titanic %>% drop_na() %>% mutate(survived = survived == "yes") %>% pull(survived), 
                       label = "Random Forest")

passenger <- titanic %>% drop_na() %>% slice(327)

bd_rf <- predict_parts(explainer = explain_titanic,
                 new_observation = passenger,
                 type = "break_down")
plot(bd_rf)
```


```{r}
mean(explain_titanic$y_hat)

titanic_data <- titanic %>% drop_na() %>% select(-survived)

titanic_all_f <- titanic_data %>% 
  mutate(gender = factor("female", levels = c("female", "male")))

predict(titanic_rf, newdata = titanic_all_f, type = "prob") %>% 
  as_tibble() %>% 
  summarize(avg_surv = mean(yes))

titanic_all_f_parch1 <- titanic_all_f %>% 
  mutate(parch = 1)

predict(titanic_rf, newdata = titanic_all_f_parch1, type = "prob") %>% 
  as_tibble() %>% 
  summarize(avg_surv = mean(yes))


predict(titanic_rf, newdata = passenger, type = "prob")
```



Contributions don't change due to which variables are considered first in lasso without interactions because our model is additive. The plot looks different, but the length of the bars is the same.

```{r}
pp_lasso_ord1 <- predict_parts(explainer = lasso_explain,
                          new_observation = new_obs,
                          type = "break_down", #default
                          order = c("bathrooms",
                                    "floors",
                                    "condition",
                                    "sqft_basement",
                                    "sqft_living",
                                    "date",
                                    "waterfront",
                                    "grade",
                                    "yr_built",
                                    "bedrooms",
                                    "sqft_lot",
                                    "view",
                                    "sqft_above",
                                    "id",
                                    "zipcode",
                                    "sqft_living15",
                                    "lat",
                                    "sqft_lot15",
                                    "yr_renovated",
                                    "long")) 

pp_lasso_ord1 

plot(pp_lasso_ord1)

# Don't judge my code here, please :)
# This is just to reorder it to make it easier to compare to the previous output

Intercept <- pp_lasso_ord1 %>% 
  as.tibble() %>% 
  slice(1)

middle <- pp_lasso_ord1 %>% 
  as.tibble() %>% 
  slice(-1, -nrow(.)) %>% 
  arrange(desc(abs(contribution)))

prediction <- pp_lasso_ord1 %>% 
  as.tibble() %>% 
  slice(nrow(.))   

Intercept %>% 
  bind_rows(middle) %>% 
  bind_rows(prediction) %>% 
  select(variable, contribution)
```


```{r}
pp_lasso_ord2 <- predict_parts(explainer = lasso_explain,
                          new_observation = new_obs,
                          type = "break_down", #default
                          order = c("yr_built",
                                    "sqft_above",
                                    "sqft_living",
                                    "floors",
                                    "condition",
                                    "date",
                                    "waterfront",
                                    "sqft_basement",
                                    "grade",
                                    "bathrooms",
                                    "bedrooms",
                                    "sqft_lot",
                                    "view",
                                    "id",
                                    "zipcode",
                                    "sqft_living15",
                                    "lat",
                                    "sqft_lot15",
                                    "yr_renovated",
                                    "long")) 

pp_lasso_ord2

plot(pp_lasso_ord2)

# The messy code again :)

Intercept2 <- pp_lasso_ord2 %>% 
  as.tibble() %>% 
  slice(1)

middle2 <- pp_lasso_ord2 %>% 
  as.tibble() %>% 
  slice(-1, -nrow(.)) %>% 
  arrange(desc(abs(contribution)))

prediction2 <- pp_lasso_ord2 %>% 
  as.tibble() %>% 
  slice(nrow(.))   

Intercept2 %>% 
  bind_rows(middle2) %>% 
  bind_rows(prediction2) %>% 
  select(variable, contribution)
```

Try for random forest ... how are variables ordered? A variable Importance measure - see [6.3](https://ema.drwhy.ai/breakDown.html#BDMethod)

```{r}
pp_rf <- predict_parts(explainer = rf_explain,
                       new_observation = new_obs,
                       type = "break_down")

pp_rf

plot(pp_rf)

pp_rf %>% 
  as_tibble()

```



What does this show?

Look at intercept bar. It is the average predicted price (*predicted*, not actual price) for all the training data (fed into the explainer). We could compute that "by hand".

```{r}

predict(ranger_fit, new_data = house_training) %>% 
  pull(.pred) %>% 
  mean()

predict(ranger_fit, new_data = new_obs)
```

Then, the `+.254` for the `grade = 10`  bar, is the average prediction if grade was fixed at 10. We could also do that "by hand".

```{r}
data_lat <- rf_explain$data %>% 
  mutate(lat = 47.6351)

predict(ranger_fit, new_data = data_lat) %>% 
  summarize(avg_lat = mean(.pred)) %>% 
  pull(avg_lat)
```



```{r}
pp_rf_ord1 <- predict_parts(explainer = rf_explain,
                            new_observation = new_obs,
                            type = "break_down", #default
                            order = c("bathrooms",
                                      "floors",
                                      "condition",
                                      "sqft_basement",
                                      "sqft_living",
                                      "date",
                                      "waterfront",
                                      "grade",
                                      "yr_built",
                                      "bedrooms",
                                      "sqft_lot",
                                      "view",
                                      "sqft_above",
                                      "id",
                                      "zipcode",
                                      "sqft_living15",
                                      "lat",
                                      "sqft_lot15",
                                      "yr_renovated",
                                      "long")) 

plot(pp_rf_ord1)

# The messy code again :)

Intercept_rf1 <- pp_rf_ord1 %>% 
  as.tibble() %>% 
  slice(1)

middle_rf1 <- pp_rf_ord1 %>% 
  as.tibble() %>% 
  slice(-1, -nrow(.)) %>% 
  arrange(desc(abs(contribution)))

prediction_rf1 <- pp_rf_ord1 %>% 
  as.tibble() %>% 
  slice(nrow(.))   

Intercept_rf1 %>% 
  bind_rows(middle_rf1) %>% 
  bind_rows(prediction_rf1) %>% 
  select(variable, contribution)
```

```{r}
pp_rf_ord2 <- predict_parts(explainer = rf_explain,
                            new_observation = new_obs,
                            type = "break_down", #default
                            order = c("yr_built",
                                      "sqft_above",
                                      "sqft_living",
                                      "floors",
                                      "condition",
                                      "date",
                                      "waterfront",
                                      "sqft_basement",
                                      "grade",
                                      "bathrooms",
                                      "bedrooms",
                                      "sqft_lot",
                                      "view",
                                      "id",
                                      "zipcode",
                                      "sqft_living15",
                                      "lat",
                                      "sqft_lot15",
                                      "yr_renovated",
                                      "long")) 

plot(pp_rf_ord2)

Intercept_rf2 <- pp_rf_ord2 %>% 
  as.tibble() %>% 
  slice(1)

middle_rf2 <- pp_rf_ord2 %>% 
  as.tibble() %>% 
  slice(-1, -nrow(.)) %>% 
  arrange(desc(abs(contribution)))

prediction_rf2 <- pp_rf_ord2 %>% 
  as.tibble() %>% 
  slice(nrow(.))   

Intercept_rf2 %>% 
  bind_rows(middle_rf2) %>% 
  bind_rows(prediction_rf2) %>% 
  select(variable, contribution)
```

